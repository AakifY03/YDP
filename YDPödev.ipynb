{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1ckvcQJWuwTz6+urxOtIk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AakifY03/YDP/blob/main/YDP%C3%B6dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Google Drive'ı bağlayın\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Veri setinin yolunu tanımlayın\n",
        "data_path = '/content/drive/My Drive/plates/'\n",
        "\n",
        "# Yolun varlığını kontrol edin\n",
        "if os.path.exists(data_path):\n",
        "    print(f\"Veriler bu dizinde mevcut: {data_path}\")\n",
        "else:\n",
        "    print(\"Veriler bulunamadı. Lütfen yolu kontrol edin.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAlzUh_Lk056",
        "outputId": "628c0aec-4aa2-4351-d785-5a7a911942a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Veriler bu dizinde mevcut: /content/drive/My Drive/plates/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "4FSnCxAGlKcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir=\"/content/drive/My Drive/plates/\"\n",
        "os.listdir(data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiqQZB1TlRyI",
        "outputId": "8ec28320-90da-40bf-ff39-fe45e35c8b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sample_submission.csv', 'test', 'train', '.DS_Store']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "train_dir = 'train'\n",
        "val_dir = 'val'\n",
        "\n",
        "class_names = ['cleaned', 'dirty']\n",
        "\n",
        "for dir_name in [train_dir, val_dir]:\n",
        "    for class_name in class_names:\n",
        "        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n",
        "\n",
        "for class_name in class_names:\n",
        "    source_dir = os.path.join(data_dir, 'train', class_name)\n",
        "    for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n",
        "        if i % 6 != 0:\n",
        "            dest_dir = os.path.join(train_dir, class_name)\n",
        "        else:\n",
        "            dest_dir = os.path.join(val_dir, class_name)\n",
        "        shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bymHJxY2l16Z",
        "outputId": "643bdde2-9ce6-449b-835a-1536e42ee51e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:04<00:00,  4.23it/s]\n",
            "100%|██████████| 21/21 [00:05<00:00,  3.94it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from torchvision import transforms, models\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(train_dir, train_transforms)\n",
        "val_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)\n",
        "\n",
        "batch_size = 8\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcMTgnv9l5ii",
        "outputId": "8ceaf5e0-289d-4f3d-fa39-adb800b53653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader), len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMqd48Q7l7-w",
        "outputId": "57fef912-9321-4382-cf8e-736d767a7020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 34)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "\n",
        "model = models.swin_t(weights=models.Swin_T_Weights.IMAGENET1K_V1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV6l31q1l-x3",
        "outputId": "1c36ad66-0321-4bb3-c6c9-617954b8e570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to /root/.cache/torch/hub/checkpoints/swin_t-704ceda3.pth\n",
            "100%|██████████| 108M/108M [00:00<00:00, 143MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "num_classes = len(train_dataset.classes)\n",
        "in_features = model.head.in_features\n",
        "model.head = nn.Linear(in_features, num_classes)"
      ],
      "metadata": {
        "id": "agbO4bM7mD1r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modeli cihaz (GPU/CPU) üzerine taşıyalım\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Kayıp fonksiyonu ve optimizasyonu ayarlayalım\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Eğitim fonksiyonu\n",
        "def train_model(model, train_dataloader, criterion, optimizer, num_epochs=10):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Eğitim moduna al\n",
        "        running_loss = 0.0\n",
        "        corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # İleriye doğru geçiş\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Geriye doğru geçiş\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            corrects += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dataloader.dataset)\n",
        "        epoch_acc = corrects.double() / total\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
        "\n",
        "        # En iyi modeli kaydet\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "z1twDGuumIcE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modeli eğitin\n",
        "model = train_model(model, train_dataloader, criterion, optimizer, num_epochs=25)\n"
      ],
      "metadata": {
        "id": "6rB1YwLvmJPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09d36a2f-c535-44f7-de60-eef6baad947c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25, Loss: 1.0930, Accuracy: 0.5588\n",
            "Epoch 2/25, Loss: 1.0997, Accuracy: 0.3235\n",
            "Epoch 3/25, Loss: 1.2394, Accuracy: 0.5000\n",
            "Epoch 4/25, Loss: 1.0261, Accuracy: 0.5588\n",
            "Epoch 5/25, Loss: 0.8991, Accuracy: 0.5294\n",
            "Epoch 6/25, Loss: 0.7795, Accuracy: 0.5294\n",
            "Epoch 7/25, Loss: 0.9104, Accuracy: 0.4118\n",
            "Epoch 8/25, Loss: 1.2379, Accuracy: 0.4118\n",
            "Epoch 9/25, Loss: 0.9249, Accuracy: 0.4412\n",
            "Epoch 10/25, Loss: 0.6647, Accuracy: 0.6176\n",
            "Epoch 11/25, Loss: 1.3063, Accuracy: 0.4412\n",
            "Epoch 12/25, Loss: 0.7889, Accuracy: 0.4706\n",
            "Epoch 13/25, Loss: 0.8464, Accuracy: 0.5588\n",
            "Epoch 14/25, Loss: 0.7116, Accuracy: 0.4706\n",
            "Epoch 15/25, Loss: 0.7642, Accuracy: 0.5000\n",
            "Epoch 16/25, Loss: 0.8415, Accuracy: 0.4706\n",
            "Epoch 17/25, Loss: 0.8103, Accuracy: 0.3529\n",
            "Epoch 18/25, Loss: 0.7241, Accuracy: 0.5294\n",
            "Epoch 19/25, Loss: 0.7377, Accuracy: 0.4118\n",
            "Epoch 20/25, Loss: 0.8088, Accuracy: 0.4706\n",
            "Epoch 21/25, Loss: 0.7430, Accuracy: 0.4706\n",
            "Epoch 22/25, Loss: 0.8351, Accuracy: 0.5000\n",
            "Epoch 23/25, Loss: 0.6963, Accuracy: 0.4706\n",
            "Epoch 24/25, Loss: 0.7063, Accuracy: 0.5882\n",
            "Epoch 25/25, Loss: 0.9399, Accuracy: 0.3824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, val_dataloader):\n",
        "    model.eval()  # Değerlendirme moduna al\n",
        "    corrects = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # No gradient calculation, sadece tahmin\n",
        "        for inputs, labels in val_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            corrects += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = corrects.double() / total\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Değerlendirmeyi başlat\n",
        "evaluate_model(model, val_dataloader)\n"
      ],
      "metadata": {
        "id": "FIKrSYCbmLhE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a7818bf-c43b-4216-bdff-f9301da0798c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JevsJfp1mN71"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}